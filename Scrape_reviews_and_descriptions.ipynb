{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"yelp-logo.png\"  width=130>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "# Scraping User Reviews from Yelp!\n",
    "\n",
    "In the notebook below, we walk through the scraping process to grab the companies reviews and the comanpy descriptions.\n",
    "\n",
    "The steps are as follows:\n",
    "- import librarys\n",
    "- create empty dictionary \n",
    "- use the fuction we defined in Mod_5_functions and scrape reviews\n",
    "- Pickle the file\n",
    "- Use the set of compnaies and scrape their descriptions\n",
    "\n",
    "\n",
    "#### Import Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from Mod_5_functions import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the empty dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_dict = {'comapny_source':[],'company_loc':[],'rev_comp_rating':[],\n",
    "               'rev_comp_reviews':[],'rev_comp_url':[],'rev_company_name':[],\n",
    "               'userUrl':[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the function get_all_reviews below.\n",
    "\n",
    "You need to specify a company url for their Yelp page to start. Because I am obsessed with Peloton, that is where I started. A few things to note, the **parameters** for this function are: **url_1, r_dict, review_num, company_count, starter_index**. \n",
    "\n",
    "*r_dict* is the dictionary we definied above, *review_num* is the number of reviewers you want to get from each company, *company_count* is the number of how many comapny sites we want to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.yelp.com/biz/peloton-new-york?osq=peloton'\n",
    "\n",
    "reviews_df = get_all_reviews(url,review_dict,review_num, company_count,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function itself will save the dictionary after it hits 100 company pages scraped. However, if you do more or less than that, then lets save it now. **Parameters: Obj (aka our dictionary) and filename.**\n",
    "\n",
    "\n",
    "Let's use the function defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file(review_dict, 'data_folder_name/filename.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get a set list of the compnaies we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp_urls = list(set(review_dict['rev_comp_url']))\n",
    "comp_names = list(set(review_dict['rev_comp_name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets now get company decriptions to help with the second part of the project: Yelp API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_reviews_train = open_pickle('Data/train_data')\n",
    "user_reviews_test = open_pickle('Data/test_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, lets create a new column in our DF called 'rev_comp_id'. We will be using regex to get the yelp company ID from the url we currently have:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43dd6c646d894cff89a7b03d499d6897"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "\n",
    "tqdm_notebook.pandas(desc=\"Progress: \")\n",
    "\n",
    "user_reviews_train['rev_comp_id'] = user_reviews_train.rev_comp_url.progress_apply(lambda url: \n",
    "                                               re.search('https\\:\\/\\/www\\.yelp\\.com\\/biz\\/(.*)', url).group(1))\n",
    "# url = re.search('https\\:\\/\\/www\\.yelp\\.com\\/biz\\/(.*)', i)\n",
    "# url.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get a list of unique IDs so we don't make multiple requests per company:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ids = list(set(user_reviews_train['rev_comp_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create an empty dictionary with the keys: ids, review_count, rating and categories and their values all empty lists. Also, input your api client id and api key as strings in the variables below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client_id = 'Your_client_id_here'\n",
    "api_key = 'Your_api_key_here'\n",
    "comp_cats = {'ids':[],'review_count':[],'rating':[], 'categories':[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, run the *get_cats_and_review* function we imported above from our functions file. Pass it the list of ids defined above:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_cats_and_review(train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file(comp_cats,'Data/company_categories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle_file(comp_cats,<span style='color:darkred'>'Data/company_categories'</span>)\n",
    "\n",
    "**Create a DataFrame from comp_cats dict, then left merge it with our original df. Left = original, right = comp_cats on ids:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp_cats_df = pd.DataFrame(comp_cats)\n",
    "training_with_cats = pd.merge(user_reviews_train, comp_cats_df, how='left', left_on='rev_comp_id', right_on='ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pickled object!'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_file(training_with_cats,'Data/training_df_with_categories')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
