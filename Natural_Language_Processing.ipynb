{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:left;margin-left: -12px; margin-top: -10px\" src=\"yelp-logo-27.png\"  width=50>\n",
    "\n",
    "# Part 2: Natural Language Processing\n",
    "\n",
    "In this notebook we will now go through the text data in the reviews and class/business descriptions. By preprocessing this data and using the NLP tools provided to us through *__Spacy__* and __*NLTK*__ we will be able to derive some meaning from the text to *hopefully* improve our models.\n",
    "\n",
    "The steps involved in this are as follows: \n",
    "\n",
    "1. word count\n",
    "2. character count\n",
    "3. Number of numerics\n",
    "4. Number of upper case\n",
    "5. Number of Exclamation Points (!)\n",
    "7. Count of stop words\n",
    "8. drop stop words\n",
    "9. lemmetize our words\n",
    "10. TF-IDF\n",
    "11. Class Imbalance and Sentiment Analysis\n",
    "\n",
    "#### Import needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elenasm7/anaconda/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pickle\n",
    "from Mod_5_functions import pickle_file,open_pickle,clean_text_column\n",
    "from nltk.corpus import stopwords\n",
    "from Mod_5_functions import return_lemma\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "%matplotlib inline\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the pickled DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_reviews_df = open_pickle('Data/filtered_user_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. word count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_reviews_df['word_count'] = user_reviews_df.rev_comp_reviews.apply(lambda x: len(str(x).split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. character count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_reviews_df['char_count'] = user_reviews_df.rev_comp_reviews.str.len() #this includes the spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Number of numerics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_reviews_df['numerics'] = user_reviews_df.rev_comp_reviews.apply(lambda x: len([x for x in x.split() if x.isdigit()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Number of upper case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_reviews_df['upper'] = user_reviews_df.rev_comp_reviews.apply(lambda x: len([x for x in x.split() if x.isupper()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Number of Exclamation Points (!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_reviews_df['bangs'] = user_reviews_df.rev_comp_reviews.apply(lambda x: len([x for x in x.split('!')]) - 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Count of stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "user_reviews_df['stp_wrd_cnt'] = user_reviews_df.rev_comp_reviews.apply(lambda x: \n",
    "                                                                        len([x for x in x.split() if x in stop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comapny_source</th>\n",
       "      <th>company_loc</th>\n",
       "      <th>rev_comp_rating</th>\n",
       "      <th>rev_comp_reviews</th>\n",
       "      <th>rev_comp_url</th>\n",
       "      <th>rev_company_name</th>\n",
       "      <th>userUrl</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "      <th>bangs</th>\n",
       "      <th>stp_wrd_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Peloton</td>\n",
       "      <td>370 Canal St New York, NY 10013</td>\n",
       "      <td>3.0</td>\n",
       "      <td>planet fitness affordable frills gym happy opt...</td>\n",
       "      <td>https://www.yelp.com/biz/planet-fitness-manhat...</td>\n",
       "      <td>Planet Fitness - Manhattan - Canal St - NY</td>\n",
       "      <td>https://www.yelp.com/user_details?userid=exPhu...</td>\n",
       "      <td>219</td>\n",
       "      <td>1189</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Peloton</td>\n",
       "      <td>90 E 10th St New York, NY 10003</td>\n",
       "      <td>2.0</td>\n",
       "      <td>purchased groupon friend calling book receptio...</td>\n",
       "      <td>https://www.yelp.com/biz/montauk-salt-cave-new...</td>\n",
       "      <td>Montauk Salt Cave</td>\n",
       "      <td>https://www.yelp.com/user_details?userid=exPhu...</td>\n",
       "      <td>791</td>\n",
       "      <td>4417</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Peloton</td>\n",
       "      <td>1841 Broadway New York, NY 11023</td>\n",
       "      <td>3.0</td>\n",
       "      <td>enjoyed class one least favorite barre studios...</td>\n",
       "      <td>https://www.yelp.com/biz/pure-barre-new-york-c...</td>\n",
       "      <td>Pure Barre - New York Columbus Circle - 60th &amp;...</td>\n",
       "      <td>https://www.yelp.com/user_details?userid=exPhu...</td>\n",
       "      <td>88</td>\n",
       "      <td>480</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  comapny_source                       company_loc  rev_comp_rating  \\\n",
       "0        Peloton   370 Canal St New York, NY 10013              3.0   \n",
       "1        Peloton   90 E 10th St New York, NY 10003              2.0   \n",
       "2        Peloton  1841 Broadway New York, NY 11023              3.0   \n",
       "\n",
       "                                    rev_comp_reviews  \\\n",
       "0  planet fitness affordable frills gym happy opt...   \n",
       "1  purchased groupon friend calling book receptio...   \n",
       "2  enjoyed class one least favorite barre studios...   \n",
       "\n",
       "                                        rev_comp_url  \\\n",
       "0  https://www.yelp.com/biz/planet-fitness-manhat...   \n",
       "1  https://www.yelp.com/biz/montauk-salt-cave-new...   \n",
       "2  https://www.yelp.com/biz/pure-barre-new-york-c...   \n",
       "\n",
       "                                    rev_company_name  \\\n",
       "0         Planet Fitness - Manhattan - Canal St - NY   \n",
       "1                                  Montauk Salt Cave   \n",
       "2  Pure Barre - New York Columbus Circle - 60th &...   \n",
       "\n",
       "                                             userUrl  word_count  char_count  \\\n",
       "0  https://www.yelp.com/user_details?userid=exPhu...         219        1189   \n",
       "1  https://www.yelp.com/user_details?userid=exPhu...         791        4417   \n",
       "2  https://www.yelp.com/user_details?userid=exPhu...          88         480   \n",
       "\n",
       "   numerics  upper  bangs  stp_wrd_cnt  \n",
       "0         0      5      0          100  \n",
       "1         2     19      4          331  \n",
       "2         0      2      0           39  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_reviews_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Next, we need to move into data cleaning. This section will be very important for the remaineder of this project and the models we run. In the next few cells we will:\n",
    "1. create a function to remove all punction\n",
    "2. lower case all of the words in our messages\n",
    "4. remove all words shorter than 3 characters\n",
    "3. remove stop words\n",
    "4. check for spelling and correct where needed\n",
    "5. remove frequent\n",
    "6. remove rare/uncommon words\n",
    "\n",
    "\n",
    "#### 1) and 2) get rid of special charaters and lower case:\n",
    "\n",
    "Use the function *clean_text_column*, which we imported above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_reviews_df.rev_comp_reviews = user_reviews_df.rev_comp_reviews.apply(lambda row: clean_text_column(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. drop stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english') #loads the stop words for the english language\n",
    "user_reviews_df.rev_comp_reviews = user_reviews_df.rev_comp_reviews.apply(lambda x: \" \".join(x for x in x.split() if x not in stop)) \n",
    "#returns only words that are not in the list of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting symspellpy\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/d5/9cf41f05a30f205c00489e3d37639c348349ba6f8d0e1005f26dc9a9ac60/symspellpy-6.3.8-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.1 in /Users/elenasm7/anaconda/lib/python3.6/site-packages (from symspellpy) (1.16.2)\n",
      "Installing collected packages: symspellpy\n",
      "Successfully installed symspellpy-6.3.8\n"
     ]
    }
   ],
   "source": [
    "!pip install -U symspellpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Correct Spelling:\n",
    "\n",
    "Check the words that have only been used once, some of these will definitly be misspelled! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of words that only occur once:\n",
    "x = [[1,2,3],[4,5,6]]\n",
    "word_lists = list(user_reviews_df.rev_comp_reviews.apply(lambda x: x.split(' ')))\n",
    "all_words = [word for rev in word_lists for word in rev]\n",
    "corpus_word_counts_df = pd.DataFrame(pd.Series(all_words).value_counts()).reset_index()\\\n",
    ".rename(columns={'index':'words',0:'counts'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_word_counts_df_1 = corpus_word_counts_df[corpus_word_counts_df['counts'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21691</th>\n",
       "      <td>firstlast</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21692</th>\n",
       "      <td>5075</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21693</th>\n",
       "      <td>strong3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21694</th>\n",
       "      <td>ladderi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21695</th>\n",
       "      <td>articulates</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             words  counts\n",
       "21691    firstlast       1\n",
       "21692         5075       1\n",
       "21693      strong3       1\n",
       "21694      ladderi       1\n",
       "21695  articulates       1"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_word_counts_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea93d6e217f45959e973d00a92dbe56"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elenasm7/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "tqdm_notebook.pandas(desc=\"Progress: \")\n",
    "\n",
    "corpus_word_counts_df_1['corrected'] = corpus_word_counts_df_1.words.progress_apply(lambda w: \n",
    "                                                                                     sym_spell.word_segmentation(w)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>counts</th>\n",
       "      <th>corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21691</th>\n",
       "      <td>firstlast</td>\n",
       "      <td>1</td>\n",
       "      <td>first last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21692</th>\n",
       "      <td>5075</td>\n",
       "      <td>1</td>\n",
       "      <td>5075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21693</th>\n",
       "      <td>strong3</td>\n",
       "      <td>1</td>\n",
       "      <td>strong 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21694</th>\n",
       "      <td>ladderi</td>\n",
       "      <td>1</td>\n",
       "      <td>ladder i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21695</th>\n",
       "      <td>articulates</td>\n",
       "      <td>1</td>\n",
       "      <td>articulates</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             words  counts    corrected\n",
       "21691    firstlast       1   first last\n",
       "21692         5075       1         5075\n",
       "21693      strong3       1     strong 3\n",
       "21694      ladderi       1     ladder i\n",
       "21695  articulates       1  articulates"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_word_counts_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_fixed_words(rev,df):\n",
    "    words = rev.split(' ')\n",
    "    cor_rev = []\n",
    "    for word in words: \n",
    "        if word in list(df.words):\n",
    "            cor_rev.append(df[df['words'] == word]['corrected'].item())\n",
    "        else:\n",
    "            cor_rev.append(word)\n",
    "    return ' '.join(cor_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pickled object!'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_file(user_reviews_df, 'Data/spelling_corrections_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13d44d6a2d347d3b07a2115bbe88be3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_reviews_df['rev_comp_reviews_corrections'] = user_reviews_df\\\n",
    ".rev_comp_reviews.progress_apply(lambda x: replace_fixed_words(x,corpus_word_counts_df_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before the spell checker we had 53476 words, and now we have: 32334\n",
      "\n",
      "31785 were corrected for\n"
     ]
    }
   ],
   "source": [
    "word_lists_new = list(user_reviews_df.rev_comp_reviews_corrections.apply(lambda x: x.split(' ')))\n",
    "all_words_corr = [word for rev in word_lists_new for word in rev]\n",
    "\n",
    "count_1= corpus_word_counts_df.shape[0]\n",
    "count_2 = pd.Series(all_words_corr).value_counts().shape[0]\n",
    "\n",
    "print(f'Before the spell checker we had {count_1} words, and now we have: {count_2}',\n",
    "      f'{corpus_word_counts_df_1.shape[0]} were corrected for',sep='\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. lemmetize our words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_lemma(review,nlp):\n",
    "    doc = nlp(review)\n",
    "    return ' '.join([word.lemma_ for word in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ea0305db6e46e99bd6e04d2b3d6eba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "user_reviews_df['rev_comp_reviews_corrections_new'] = user_reviews_df.rev_comp_reviews_corrections.\\\n",
    "progress_apply(lambda x: return_lemma(x,nlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pickled object!'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_file(user_reviews_df, 'Data/sp_and_lemm_df')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    planet fitness affordable frills gym happy opt...\n",
       "1    purchased groupon friend calling book receptio...\n",
       "2    enjoyed class one least favorite barre studios...\n",
       "Name: rev_comp_reviews_corrections, dtype: object"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_reviews_df.rev_comp_reviews_corrections[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(user_reviews_df.rev_comp_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [word for rev in user_reviews_df.rev_comp_reviews for word in rev.split(' ')]\n",
    "len(set(test)), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = pd.Series(test)\n",
    "tester.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPDATE!!!!\n",
    "\n",
    "We can see that out of the **911876** words in our corpus (all the words in all of the reviews), only **50 of the words are unique**. That is pretty crazy, but also teling of the reviews being left. We can use this to our advantage. With such a small list of words lets explore them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "words \n",
    "sns.countplot(test,color='red')\n",
    "plt.title('Unique Word Count',fontsize=15)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.xlabel('Unique Review Words');\n",
    "# sns.set(font_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Class Imbalance and Sentiment Analysis:\n",
    "\n",
    "If we look at the distributions above we can see that some users appear more frequently than others, but also our star ratings are positively skewed. \n",
    "\n",
    "In the next few cells we will try address this issue by augemneting the scores given by users with the sentiment scores of their reviews. \n",
    "\n",
    "**An example of how this works:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "ss = sid.polarity_scores('I love this place so much! It is my favoirte place ever!!')\n",
    "ss_2 = sid.polarity_scores('this place is literally the worst ever, it deserves a zero!')\n",
    "ss_3 = sid.polarity_scores('this place is average, got the job done.')\n",
    "ss_4 = sid.polarity_scores(users_reviews_df.rev_comp_reviews[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(f'I love this place so much! It is my favoirte place ever!!: {ss}',\n",
    "     f'this place is literally the worst ever, it deserves a zero!: {ss_2}',\n",
    "     f'this place is average, got the job done.: {ss_3}',\n",
    "     f'{users_reviews_df.rev_comp_reviews[100]}: {ss_4}', sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on the results of this exploration, maybe the best way to handle this is the following:**\n",
    "- subtract the negative score from the positive score\n",
    "- multiply this by the original rating\n",
    "- add this new rating to our original rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_reviews_df['sentiment_score'] = users_reviews_df.rev_comp_reviews.apply(lambda rev: sid.polarity_scores(rev))\n",
    "users_reviews_df['pos_neg'] = users_reviews_df.sentiment_score.apply(lambda sent: sent['pos']-sent['neg'])\n",
    "users_reviews_df['new_rating'] = users_reviews_df.pos_neg*users_filter.rev_comp_rating + users_filter.rev_comp_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(users_filter['new_rating'],color='red',bins=15)\n",
    "plt.title('New Star Rating Histogram',fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test Spilt:\n",
    "\n",
    "there's no good way that's been suggested on the internet to test your models. So, here we will remove a few users with multiple reviews so we can test on them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot_users = len(set(users_filter.userUrl))\n",
    "print(f'There are a total of {tot_users} users, and 20% of that is {int(tot_users*.2)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is the random list of users we will be selecting and grabbing the usersID and then grabbing all of their rows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_select = np.random.randint(0,15722,766)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "citations:\n",
    "\n",
    "\n",
    "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n",
    "\n",
    "https://opensourceforu.com/2016/12/analysing-sentiments-nltk/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
